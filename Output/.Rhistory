colnames(peak.segments)= c("Low.boundary","High.boundary","Max_rate_mg_per_L_per_h","Mid.peaks","Total_number_of_peaks","X00681_NPOC_mg_per_L_as_C", "RichoverDOC" ,"Sample_ID")
temp.low = min(all.field.peaks$RichoverDOC)
temp.high = temp.low + field.step
temp.dat = all.field.peaks[which(all.field.peaks$X00681_NPOC_mg_per_L_as_C >= temp.low & all.field.peaks$RichoverDOC <= temp.high),]
temp.rate = max(all.field.peaks$rate_mg_per_L_per_h[which(all.field.peaks$RichoverDOC >= temp.low & all.field.peaks$RichoverDOC <= temp.high)])
temp.mid =  temp.dat$RichoverDOC[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
View(temp.dat)
i
i = 1
temp.high = temp.low + field.step
temp.dat = all.field.peaks[which(all.field.peaks$X00681_NPOC_mg_per_L_as_C >= temp.low & all.field.peaks$RichoverDOC <= temp.high),]
ideal.num.segments = 10
all.field.peaks = all.field[order(all.field$RichoverDOC),]
field.step = (max(all.field.peaks$RichoverDOC)- min(all.field.peaks$RichoverDOC))/(ideal.num.segments)
peak.segments = as.data.frame(matrix(NA,ncol = 8,nrow = ideal.num.segments))
colnames(peak.segments)= c("Low.boundary","High.boundary","Max_rate_mg_per_L_per_h","Mid.peaks","Total_number_of_peaks","X00681_NPOC_mg_per_L_as_C", "RichoverDOC" ,"Sample_ID")
View(all.field.peaks)
temp.low = min(all.field.peaks$RichoverDOC)
peak.segments = as.data.frame(matrix(NA,ncol = 8,nrow = ideal.num.segments))
colnames(peak.segments)= c("Low.boundary","High.boundary","Max_rate_mg_per_L_per_h","Mid.peaks","Total_number_of_peaks","X00681_NPOC_mg_per_L_as_C", "RichoverDOC" ,"Sample_ID")
temp.low = min(all.field.peaks$RichoverDOC)
i
temp.high = temp.low + field.step
temp.dat = all.field.peaks[which(all.field.peaks$X00681_NPOC_mg_per_L_as_C >= temp.low & all.field.peaks$RichoverDOC <= temp.high),]
all.field.peaks$RichoverDOC
[which(all.field.peaks$X00681_NPOC_mg_per_L_as_C >= temp.low & all.field.peaks$RichoverDOC <= temp.high),]
which(all.field.peaks$X00681_NPOC_mg_per_L_as_C >= temp.low & all.field.peaks$RichoverDOC <= temp.high)
# Field
# Defining the segments
ideal.num.segments = 10
all.field.peaks = all.field[order(all.field$RichoverDOC),]
field.step = (max(all.field.peaks$RichoverDOC)- min(all.field.peaks$RichoverDOC))/(ideal.num.segments)
peak.segments = as.data.frame(matrix(NA,ncol = 8,nrow = ideal.num.segments))
colnames(peak.segments)= c("Low.boundary","High.boundary","Max_rate_mg_per_L_per_h","Mid.peaks","Total_number_of_peaks","X00681_NPOC_mg_per_L_as_C", "RichoverDOC" ,"Sample_ID")
temp.low = min(all.field.peaks$RichoverDOC)
for (i in 1:ideal.num.segments){
temp.high = temp.low + field.step
temp.dat = all.field.peaks[which(all.field.peaks$RichoverDOC >= temp.low & all.field.peaks$RichoverDOC <= temp.high),]
if (nrow(temp.dat) > 0){
temp.rate = max(all.field.peaks$rate_mg_per_L_per_h[which(all.field.peaks$RichoverDOC >= temp.low & all.field.peaks$RichoverDOC <= temp.high)])
temp.mid =  temp.dat$RichoverDOC[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$Low.boundary[i] = temp.low
peak.segments$High.boundary[i] = temp.high
peak.segments$Max_rate_mg_per_L_per_h[i] = temp.rate
peak.segments$Mid.peaks[i] = temp.mid
peak.segments$Total_number_of_peaks[i] = temp.dat$Total_number_of_peaks[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$X00681_NPOC_mg_per_L_as_C[i] = temp.dat$X00681_NPOC_mg_per_L_as_C[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$Sample_ID[i] = temp.dat$Sample_ID[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$RichoverDOC[i] = peak.segments$Total_number_of_peaks[i]/peak.segments$X00681_NPOC_mg_per_L_as_C[i]
temp.low = temp.high
} else if (nrow(temp.dat) == 0){
temp.rate = NA
temp.mid =  NA
peak.segments$Low.boundary[i] = temp.low
peak.segments$High.boundary[i] = temp.high
peak.segments$Max_rate_mg_per_L_per_h[i] = temp.rate
peak.segments$Mid.peaks[i] = temp.mid
peak.segments$Total_number_of_peaks[i] = NA
peak.segments$X00681_NPOC_mg_per_L_as_C[i] = NA
peak.segments$Sample_ID[i] = NA
peak.segments$RichoverDOC[i] = NA
temp.low = temp.high
}
}
View(peak.segments.incubation)
peak.segments$overNPOC = 1 / peak.segments$X00681_NPOC_mg_per_L_as_C
summary(lm(log(peak.segments$Max_rate_mg_per_L_per_h) ~
peak.segments$Total_number_of_peaks +
peak.segments$overNPOC +
peak.segments$RichoverDOC))
AIC(lm(log(peak.segments$Max_rate_mg_per_L_per_h) ~
peak.segments$Total_number_of_peaks +
peak.segments$overNPOC +
peak.segments$RichoverDOC))
rm(peak.segments)
peak.segments$overNPOC = 1 / peak.segments$X00681_NPOC_mg_per_L_as_C
# Field
# Defining the segments
ideal.num.segments = 10
all.field.peaks = all.field[order(all.field$RichoverDOC),]
field.step = (max(all.field.peaks$RichoverDOC)- min(all.field.peaks$RichoverDOC))/(ideal.num.segments)
peak.segments = as.data.frame(matrix(NA,ncol = 8,nrow = ideal.num.segments))
colnames(peak.segments)= c("Low.boundary","High.boundary","Max_rate_mg_per_L_per_h","Mid.peaks","Total_number_of_peaks","X00681_NPOC_mg_per_L_as_C", "RichoverDOC" ,"Sample_ID")
temp.low = min(all.field.peaks$RichoverDOC)
for (i in 1:ideal.num.segments){
temp.high = temp.low + field.step
temp.dat = all.field.peaks[which(all.field.peaks$RichoverDOC >= temp.low & all.field.peaks$RichoverDOC <= temp.high),]
if (nrow(temp.dat) > 0){
temp.rate = max(all.field.peaks$rate_mg_per_L_per_h[which(all.field.peaks$RichoverDOC >= temp.low & all.field.peaks$RichoverDOC <= temp.high)])
temp.mid =  temp.dat$RichoverDOC[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$Low.boundary[i] = temp.low
peak.segments$High.boundary[i] = temp.high
peak.segments$Max_rate_mg_per_L_per_h[i] = temp.rate
peak.segments$Mid.peaks[i] = temp.mid
peak.segments$Total_number_of_peaks[i] = temp.dat$Total_number_of_peaks[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$X00681_NPOC_mg_per_L_as_C[i] = temp.dat$X00681_NPOC_mg_per_L_as_C[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$Sample_ID[i] = temp.dat$Sample_ID[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$RichoverDOC[i] = peak.segments$Total_number_of_peaks[i]/peak.segments$X00681_NPOC_mg_per_L_as_C[i]
temp.low = temp.high
} else if (nrow(temp.dat) == 0){
temp.rate = NA
temp.mid =  NA
peak.segments$Low.boundary[i] = temp.low
peak.segments$High.boundary[i] = temp.high
peak.segments$Max_rate_mg_per_L_per_h[i] = temp.rate
peak.segments$Mid.peaks[i] = temp.mid
peak.segments$Total_number_of_peaks[i] = NA
peak.segments$X00681_NPOC_mg_per_L_as_C[i] = NA
peak.segments$Sample_ID[i] = NA
peak.segments$RichoverDOC[i] = NA
temp.low = temp.high
}
}
peak.segments$overNPOC = 1 / peak.segments$X00681_NPOC_mg_per_L_as_C
summary(lm(log(peak.segments$Max_rate_mg_per_L_per_h) ~
peak.segments$Total_number_of_peaks +
peak.segments$overNPOC +
peak.segments$RichoverDOC))
AIC = AIC(lm(log(peak.segments$Max_rate_mg_per_L_per_h) ~
peak.segments$Total_number_of_peaks +
peak.segments$overNPOC +
peak.segments$RichoverDOC))
AIC
rm(peak.segments)
# Field
# Defining the segments
ideal.num.segments = 10
all.field.peaks = all.field[order(all.field$RichoverDOC),]
field.step = (max(all.field.peaks$RichoverDOC)- min(all.field.peaks$RichoverDOC))/(ideal.num.segments)
peak.segments = as.data.frame(matrix(NA,ncol = 8,nrow = ideal.num.segments))
colnames(peak.segments)= c("Low.boundary","High.boundary","Max_rate_mg_per_L_per_h","Mid.peaks","Total_number_of_peaks","X00681_NPOC_mg_per_L_as_C", "RichoverDOC" ,"Sample_ID")
temp.low = min(all.field.peaks$RichoverDOC)
for (i in 1:ideal.num.segments){
temp.high = temp.low + field.step
temp.dat = all.field.peaks[which(all.field.peaks$RichoverDOC >= temp.low & all.field.peaks$RichoverDOC <= temp.high),]
if (nrow(temp.dat) > 0){
temp.rate = max(all.field.peaks$rate_mg_per_L_per_h[which(all.field.peaks$RichoverDOC >= temp.low & all.field.peaks$RichoverDOC <= temp.high)])
temp.mid =  temp.dat$RichoverDOC[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$Low.boundary[i] = temp.low
peak.segments$High.boundary[i] = temp.high
peak.segments$Max_rate_mg_per_L_per_h[i] = temp.rate
peak.segments$Mid.peaks[i] = temp.mid
peak.segments$Total_number_of_peaks[i] = temp.dat$Total_number_of_peaks[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$X00681_NPOC_mg_per_L_as_C[i] = temp.dat$X00681_NPOC_mg_per_L_as_C[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$Sample_ID[i] = temp.dat$Sample_ID[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$RichoverDOC[i] = peak.segments$Total_number_of_peaks[i]/peak.segments$X00681_NPOC_mg_per_L_as_C[i]
temp.low = temp.high
} else if (nrow(temp.dat) == 0){
temp.rate = NA
temp.mid =  NA
peak.segments$Low.boundary[i] = temp.low
peak.segments$High.boundary[i] = temp.high
peak.segments$Max_rate_mg_per_L_per_h[i] = temp.rate
peak.segments$Mid.peaks[i] = temp.mid
peak.segments$Total_number_of_peaks[i] = NA
peak.segments$X00681_NPOC_mg_per_L_as_C[i] = NA
peak.segments$Sample_ID[i] = NA
peak.segments$RichoverDOC[i] = NA
temp.low = temp.high
}
}
peak.segments$overNPOC = 1 / peak.segments$X00681_NPOC_mg_per_L_as_C
summary(lm(log(peak.segments$Max_rate_mg_per_L_per_h) ~
peak.segments$Total_number_of_peaks +
peak.segments$overNPOC +
peak.segments$RichoverDOC))
AIC = AIC(lm(log(peak.segments$Max_rate_mg_per_L_per_h) ~
peak.segments$Total_number_of_peaks +
peak.segments$overNPOC +
peak.segments$RichoverDOC))
print("AIC")
AIC
rm(peak.segments)
# Field
# Defining the segments
ideal.num.segments = 10
all.field.peaks = all.field[order(all.field$RichoverDOC),]
field.step = (max(all.field.peaks$RichoverDOC)- min(all.field.peaks$RichoverDOC))/(ideal.num.segments)
peak.segments = as.data.frame(matrix(NA,ncol = 8,nrow = ideal.num.segments))
colnames(peak.segments)= c("Low.boundary","High.boundary","Max_rate_mg_per_L_per_h","Mid.peaks","Total_number_of_peaks","X00681_NPOC_mg_per_L_as_C", "RichoverDOC" ,"Sample_ID")
temp.low = min(all.field.peaks$RichoverDOC)
for (i in 1:ideal.num.segments){
temp.high = temp.low + field.step
temp.dat = all.field.peaks[which(all.field.peaks$RichoverDOC >= temp.low & all.field.peaks$RichoverDOC <= temp.high),]
if (nrow(temp.dat) > 0){
temp.rate = max(all.field.peaks$rate_mg_per_L_per_h[which(all.field.peaks$RichoverDOC >= temp.low & all.field.peaks$RichoverDOC <= temp.high)])
temp.mid =  temp.dat$RichoverDOC[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$Low.boundary[i] = temp.low
peak.segments$High.boundary[i] = temp.high
peak.segments$Max_rate_mg_per_L_per_h[i] = temp.rate
peak.segments$Mid.peaks[i] = temp.mid
peak.segments$Total_number_of_peaks[i] = temp.dat$Total_number_of_peaks[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$X00681_NPOC_mg_per_L_as_C[i] = temp.dat$X00681_NPOC_mg_per_L_as_C[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$Sample_ID[i] = temp.dat$Sample_ID[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$RichoverDOC[i] = peak.segments$Total_number_of_peaks[i]/peak.segments$X00681_NPOC_mg_per_L_as_C[i]
temp.low = temp.high
} else if (nrow(temp.dat) == 0){
temp.rate = NA
temp.mid =  NA
peak.segments$Low.boundary[i] = temp.low
peak.segments$High.boundary[i] = temp.high
peak.segments$Max_rate_mg_per_L_per_h[i] = temp.rate
peak.segments$Mid.peaks[i] = temp.mid
peak.segments$Total_number_of_peaks[i] = NA
peak.segments$X00681_NPOC_mg_per_L_as_C[i] = NA
peak.segments$Sample_ID[i] = NA
peak.segments$RichoverDOC[i] = NA
temp.low = temp.high
}
}
peak.segments$overNPOC = 1 / peak.segments$X00681_NPOC_mg_per_L_as_C
summary(lm(log(peak.segments$Max_rate_mg_per_L_per_h) ~
peak.segments$Total_number_of_peaks +
peak.segments$overNPOC +
peak.segments$RichoverDOC))
AIC = AIC(lm(log(peak.segments$Max_rate_mg_per_L_per_h) ~
peak.segments$Total_number_of_peaks +
peak.segments$overNPOC +
peak.segments$RichoverDOC))
print(AIC)
AIC
rm(peak.segments)
# Field
# Defining the segments
ideal.num.segments = 10
all.field.peaks = all.field[order(all.field$RichoverDOC),]
field.step = (max(all.field.peaks$RichoverDOC)- min(all.field.peaks$RichoverDOC))/(ideal.num.segments)
peak.segments = as.data.frame(matrix(NA,ncol = 8,nrow = ideal.num.segments))
colnames(peak.segments)= c("Low.boundary","High.boundary","Max_rate_mg_per_L_per_h","Mid.peaks","Total_number_of_peaks","X00681_NPOC_mg_per_L_as_C", "RichoverDOC" ,"Sample_ID")
temp.low = min(all.field.peaks$RichoverDOC)
for (i in 1:ideal.num.segments){
temp.high = temp.low + field.step
temp.dat = all.field.peaks[which(all.field.peaks$RichoverDOC >= temp.low & all.field.peaks$RichoverDOC <= temp.high),]
if (nrow(temp.dat) > 0){
temp.rate = max(all.field.peaks$rate_mg_per_L_per_h[which(all.field.peaks$RichoverDOC >= temp.low & all.field.peaks$RichoverDOC <= temp.high)])
temp.mid =  temp.dat$RichoverDOC[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$Low.boundary[i] = temp.low
peak.segments$High.boundary[i] = temp.high
peak.segments$Max_rate_mg_per_L_per_h[i] = temp.rate
peak.segments$Mid.peaks[i] = temp.mid
peak.segments$Total_number_of_peaks[i] = temp.dat$Total_number_of_peaks[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$X00681_NPOC_mg_per_L_as_C[i] = temp.dat$X00681_NPOC_mg_per_L_as_C[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$Sample_ID[i] = temp.dat$Sample_ID[which(temp.dat$rate_mg_per_L_per_h %in% temp.rate)]
peak.segments$RichoverDOC[i] = peak.segments$Total_number_of_peaks[i]/peak.segments$X00681_NPOC_mg_per_L_as_C[i]
temp.low = temp.high
} else if (nrow(temp.dat) == 0){
temp.rate = NA
temp.mid =  NA
peak.segments$Low.boundary[i] = temp.low
peak.segments$High.boundary[i] = temp.high
peak.segments$Max_rate_mg_per_L_per_h[i] = temp.rate
peak.segments$Mid.peaks[i] = temp.mid
peak.segments$Total_number_of_peaks[i] = NA
peak.segments$X00681_NPOC_mg_per_L_as_C[i] = NA
peak.segments$Sample_ID[i] = NA
peak.segments$RichoverDOC[i] = NA
temp.low = temp.high
}
}
View(peak.segments)
install.packages(c("ggsn", "sf"))
knitr::opts_chunk$set(echo = TRUE)
library(devtools)
library(ggbiplot)
library(tidymodels)
library(randomForest)
library(ranger)
library(vip)
rm(list=ls());graphics.off()
input.path = "//pnl/Projects/ECA_Project/VGC/02_Consortium_reviews/Inputs/"
output.path = "//pnl/Projects/ECA_Project/VGC/02_Consortium_reviews/Outputs/"
setwd(input.path)
# Read in data
traits = read.csv("Aggregate_results_GFE_and_lambda_2-4_clean.csv")
features = read.csv("Features_PCA_Scores_PC7.csv")
names(features)[1] = "Site_ID"
# Separating traits into sediment and water
# Sediments
traits.sediment = traits[grep("Sed_Field",traits$Sample_ID),]
traits.sediment$Site_ID = gsub("_Sed_Field_ICR","",traits.sediment$Sample_ID)
# Water
traits.water = traits[!grepl("Sed_Field",traits$Sample_ID),]
traits.water$Site_ID = gsub("_ICR","",traits.water$Sample_ID)
lambda.df = cbind.data.frame(Site_ID = traits.sediment$Site_ID,
Median_lambda = traits.sediment$Medianlambda)
data = merge(lambda.df,features, by = "Site_ID")
data = data[,-1]
# Create a balanced data split
data.split = initial_split(data, strata = Median_lambda, prob = 0.8)
# Change the prob parameter of the percentage
#if the training and testing data need to be changed
data.training = training(data.split)
data.testing = testing(data.split)
View(features)
View(data)
?rand_forest
citation('streamMetabolizer')
rm(list=ls());graphics.off()
#Loading packages
library(ggplot2);library(vegan)
library(ggthemes);library(ggpubr);library(dplyr); library(stringr);library(ggpmisc)
#####################################################################################
# Set directories
home.dir = "C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SPS2/Data/"
output.dir = ("C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SPS2/Output/")
setwd(output.dir) # setting working directory to the output directory
geo.input = paste0(home.dir,"Geospatial_data/")
shp.input = paste0(home.dir,"Shape_files_RC_model/")
# Load data
model.out = read.csv(paste0(geo.input,"RC2_model_inputs_outputs_all.csv"))
model.out.2 = read.csv(paste0(geo.input,"RC2_model_inputs_outputs_all_v2.csv"))
stream.out = read.csv(paste0(geo.input,"RC2_all_stream_Attributes.csv"))
topo = read.csv(paste0(geo.input,"RC2_all_topographic_Attributes.csv"))
ppl = read.csv(paste0(geo.input,"RC2_all_Population_Regional_WaterUse_Attributes.csv"))
land = read.csv(paste0(geo.input,"RC2_all_LandCover_Attributes.csv"))
hydro = read.csv(paste0(geo.input,"RC2_all_Hydrologic_Attributes.csv"))
climate = read.csv(paste0(geo.input,"RC2_all_Climateand WaterBalanceModel.csv"))
loc = read.csv(paste0(geo.input,"022522_all_sites_NHD_streamline_matched_v2.csv"))
slope = read.csv(paste0(geo.input,"RC2_all_slope.csv"))
slope_hr = read.csv(paste0(geo.input,"RC2_all_slope_10_m.csv"))
slope_hr = read.csv(paste0(geo.input,"RC2_all_slope_10_m.csv"))
rm(list=ls());graphics.off()
#Loading packages
library(ggplot2);library(vegan)
library(ggthemes);library(ggpubr);library(dplyr); library(stringr);library(ggpmisc)
#####################################################################################
# Set directories
home.dir = "C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SPS2/Data/"
output.dir = ("C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SPS2/Output/")
setwd(output.dir) # setting working directory to the output directory
geo.input = paste0(home.dir,"Geospatial_data/")
shp.input = paste0(home.dir,"Shape_files_RC_model/")
# Load data
model.out = read.csv(paste0(geo.input,"RC2_model_inputs_outputs_all.csv"))
model.out.2 = read.csv(paste0(geo.input,"RC2_model_inputs_outputs_all_v2.csv"))
stream.out = read.csv(paste0(geo.input,"RC2_all_stream_Attributes.csv"))
topo = read.csv(paste0(geo.input,"RC2_all_topographic_Attributes.csv"))
ppl = read.csv(paste0(geo.input,"RC2_all_Population_Regional_WaterUse_Attributes.csv"))
land = read.csv(paste0(geo.input,"RC2_all_LandCover_Attributes.csv"))
hydro = read.csv(paste0(geo.input,"RC2_all_Hydrologic_Attributes.csv"))
climate = read.csv(paste0(geo.input,"RC2_all_Climateand WaterBalanceModel.csv"))
loc = read.csv(paste0(geo.input,"022522_all_sites_NHD_streamline_matched_v2.csv"))
slope_hr = read.csv(paste0(geo.input,"RC2_all_slope_10_m.csv"))
df = Reduce(function(x,y)merge(x,y, all = T, by = c('site_ID','COMID','ID')),list(topo,stream.out,climate,hydro,land,ppl,model.out.2)) #
colnames(df)= gsub("StreamOrde","StreamOrder",colnames(df))
df = Reduce(function(x,y)merge(x,y, all = T, by = c('site_ID','COMID','ID')),list(topo,stream.out,climate,hydro,land,ppl,model.out.2)) #
colnames(df)= gsub("StreamOrde","StreamOrder",colnames(df))
df1 = Reduce(function(x,y)merge(x,y, all = T, by = c('site_ID')),list(loc,df)) #
View(df1)
df$site_ID = gsub(" ","",df$site_ID)
View(df)
gsub(" ","",df$site_ID)
df$site_ID = gsub("0 ","",df$site_ID)
df$site_ID[1]
df$site_ID = gsub("0 ","0",df$site_ID)
gsub("0 ","0",df$site_ID[1])
gsub(pattern = "0 ","0",df$site_ID[1])
gsub(pattern = "1 ","0",df$site_ID[2])
str_replace_all(df$site_ID[2], " ", "")
df$site_ID
trimws(df$site_ID[1])
View(df)
rm(list=ls());graphics.off()
#Loading packages
library(ggplot2);library(vegan)
library(ggthemes);library(ggpubr);library(dplyr); library(stringr);library(ggpmisc)
#####################################################################################
# Set directories
home.dir = "C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SPS2/Data/"
output.dir = ("C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SPS2/Output/")
setwd(output.dir) # setting working directory to the output directory
geo.input = paste0(home.dir,"Geospatial_data/")
shp.input = paste0(home.dir,"Shape_files_RC_model/")
# Load data
model.out = read.csv(paste0(geo.input,"RC2_model_inputs_outputs_all.csv"))
model.out.2 = read.csv(paste0(geo.input,"RC2_model_inputs_outputs_all_v2.csv"))
stream.out = read.csv(paste0(geo.input,"RC2_all_stream_Attributes.csv"))
topo = read.csv(paste0(geo.input,"RC2_all_topographic_Attributes.csv"))
ppl = read.csv(paste0(geo.input,"RC2_all_Population_Regional_WaterUse_Attributes.csv"))
land = read.csv(paste0(geo.input,"RC2_all_LandCover_Attributes.csv"))
hydro = read.csv(paste0(geo.input,"RC2_all_Hydrologic_Attributes.csv"))
climate = read.csv(paste0(geo.input,"RC2_all_Climateand WaterBalanceModel.csv"))
loc = read.csv(paste0(geo.input,"022522_all_sites_NHD_streamline_matched_v2.csv"))
slope_hr = read.csv(paste0(geo.input,"RC2_all_slope_10_m.csv"))
# Merge all the dataframes by site_ID
df = Reduce(function(x,y)merge(x,y, all = T, by = c('site_ID','COMID','ID')),list(topo,stream.out,climate,hydro,land,ppl,model.out.2)) #
colnames(df)= gsub("StreamOrde","StreamOrder",colnames(df))
df1 = Reduce(function(x,y)merge(x,y, all = T, by = c('site_ID')),list(loc,df)) #
rm(list=ls());graphics.off()
#Loading packages
library(ggplot2);library(vegan)
library(ggthemes);library(ggpubr);library(dplyr); library(stringr);library(ggpmisc)
#####################################################################################
# Set directories
home.dir = "C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SPS2/Data/"
output.dir = ("C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SPS2/Output/")
setwd(output.dir) # setting working directory to the output directory
geo.input = paste0(home.dir,"Geospatial_data/")
shp.input = paste0(home.dir,"Shape_files_RC_model/")
# Load data
model.out = read.csv(paste0(geo.input,"RC2_model_inputs_outputs_all.csv"))
model.out.2 = read.csv(paste0(geo.input,"RC2_model_inputs_outputs_all_v2.csv"))
stream.out = read.csv(paste0(geo.input,"RC2_all_stream_Attributes.csv"))
topo = read.csv(paste0(geo.input,"RC2_all_topographic_Attributes.csv"))
ppl = read.csv(paste0(geo.input,"RC2_all_Population_Regional_WaterUse_Attributes.csv"))
land = read.csv(paste0(geo.input,"RC2_all_LandCover_Attributes.csv"))
hydro = read.csv(paste0(geo.input,"RC2_all_Hydrologic_Attributes.csv"))
climate = read.csv(paste0(geo.input,"RC2_all_Climateand WaterBalanceModel.csv"))
loc = read.csv(paste0(geo.input,"022522_all_sites_NHD_streamline_matched_v2.csv"))
slope_hr = read.csv(paste0(geo.input,"RC2_all_slope_10_m.csv"))
# Merge all the dataframes by site_ID
df = Reduce(function(x,y)merge(x,y, all = T, by = c('site_ID','COMID','ID')),list(topo,stream.out,climate,hydro,land,ppl,model.out.2)) #
colnames(df)= gsub("StreamOrde","StreamOrder",colnames(df))
df1 = Reduce(function(x,y)merge(x,y, all = T, by = c('site_ID')),list(loc,df)) #
View(df1)
gsub("Â","",df$site_ID[1])
sub("\\s+$", "", Bdf$ste_ID[1])
sub("\\s+$", "", df$site_ID[1])
sub("\\s+$", "AAA", df$site_ID[1])
substring(df$site_ID[1],1,3)
df$site_ID[1] = substring(df$site_ID[1],1,3)
substring(df$site_ID[2],1,3)
substring(df$site_ID[3],1,3)
substring(df$site_ID[4],1,3)
substring(df$site_ID[5],1,3)
substring(df$site_ID[5],1,4)
substring(df$site_ID[5],1,5)
df$site_ID
substring(df$site_ID[101],1,3)
substring(df$site_ID[103],1,3)
substring(df$site_ID[105],1,3)
rm(list=ls());graphics.off()
#Loading packages
library(ggplot2);library(vegan)
library(ggthemes);library(ggpubr);library(dplyr); library(stringr);library(ggpmisc)
#####################################################################################
# Set directories
home.dir = "C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SPS2/Data/"
output.dir = ("C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SPS2/Output/")
setwd(output.dir) # setting working directory to the output directory
geo.input = paste0(home.dir,"Geospatial_data/")
shp.input = paste0(home.dir,"Shape_files_RC_model/")
# Load data
model.out = read.csv(paste0(geo.input,"RC2_model_inputs_outputs_all.csv"))
model.out.2 = read.csv(paste0(geo.input,"RC2_model_inputs_outputs_all_v2.csv"))
stream.out = read.csv(paste0(geo.input,"RC2_all_stream_Attributes.csv"))
topo = read.csv(paste0(geo.input,"RC2_all_topographic_Attributes.csv"))
ppl = read.csv(paste0(geo.input,"RC2_all_Population_Regional_WaterUse_Attributes.csv"))
land = read.csv(paste0(geo.input,"RC2_all_LandCover_Attributes.csv"))
hydro = read.csv(paste0(geo.input,"RC2_all_Hydrologic_Attributes.csv"))
climate = read.csv(paste0(geo.input,"RC2_all_Climateand WaterBalanceModel.csv"))
loc = read.csv(paste0(geo.input,"022522_all_sites_NHD_streamline_matched_v2.csv"))
slope_hr = read.csv(paste0(geo.input,"RC2_all_slope_10_m.csv"))
# Merge all the dataframes by site_ID
df = Reduce(function(x,y)merge(x,y, all = T, by = c('site_ID','COMID','ID')),list(topo,stream.out,climate,hydro,land,ppl,model.out.2)) #
colnames(df)= gsub("StreamOrde","StreamOrder",colnames(df))
df$site_ID[1] = substring(df$site_ID[1],1,3)
df$site_ID[2] = substring(df$site_ID[2],1,3)
df$site_ID[3] = substring(df$site_ID[3],1,3)
df$site_ID[4] = substring(df$site_ID[4],1,3)
df$site_ID[5] = substring(df$site_ID[5],1,4)
df$site_ID[6] = substring(df$site_ID[6],1,4)
df$site_ID[101] = substring(df$site_ID[101],1,3)
df$site_ID[102] = substring(df$site_ID[102],1,3)
df$site_ID[103] = substring(df$site_ID[103],1,3)
df$site_ID[104] = substring(df$site_ID[104],1,3)
df$site_ID[105] = substring(df$site_ID[105],1,3)
df$site_ID[109] = substring(df$site_ID[109],1,3)
df$site_ID[110] = substring(df$site_ID[110],1,3)
df1 = Reduce(function(x,y)merge(x,y, all = T, by = c('site_ID')),list(loc,df)) #
View(df1)
rm(list=ls());graphics.off()
#Loading packages
library(ggplot2);library(vegan)
library(ggthemes);library(ggpubr);library(dplyr); library(stringr);library(ggpmisc)
#####################################################################################
# Set directories
home.dir = "C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SPS2/Data/"
output.dir = ("C:/Users/gara009/OneDrive - PNNL/Documents/GitHub/SPS2/Output/")
setwd(output.dir) # setting working directory to the output directory
geo.input = paste0(home.dir,"Geospatial_data/")
shp.input = paste0(home.dir,"Shape_files_RC_model/")
# Load data
model.out = read.csv(paste0(geo.input,"RC2_model_inputs_outputs_all.csv"))
model.out.2 = read.csv(paste0(geo.input,"RC2_model_inputs_outputs_all_v2.csv"))
stream.out = read.csv(paste0(geo.input,"RC2_all_stream_Attributes.csv"))
topo = read.csv(paste0(geo.input,"RC2_all_topographic_Attributes.csv"))
ppl = read.csv(paste0(geo.input,"RC2_all_Population_Regional_WaterUse_Attributes.csv"))
land = read.csv(paste0(geo.input,"RC2_all_LandCover_Attributes.csv"))
hydro = read.csv(paste0(geo.input,"RC2_all_Hydrologic_Attributes.csv"))
climate = read.csv(paste0(geo.input,"RC2_all_Climateand WaterBalanceModel.csv"))
loc = read.csv(paste0(geo.input,"022522_all_sites_NHD_streamline_matched_v2.csv"))
slope_hr = read.csv(paste0(geo.input,"RC2_all_slope_10_m.csv"))
# Merge all the dataframes by site_ID
df = Reduce(function(x,y)merge(x,y, all = T, by = c('site_ID','COMID','ID')),list(topo,stream.out,climate,hydro,land,ppl,model.out.2)) #
colnames(df)= gsub("StreamOrde","StreamOrder",colnames(df))
df$site_ID[1] = substring(df$site_ID[1],1,3)
df$site_ID[2] = substring(df$site_ID[2],1,3)
df$site_ID[3] = substring(df$site_ID[3],1,3)
df$site_ID[4] = substring(df$site_ID[4],1,3)
df$site_ID[5] = substring(df$site_ID[5],1,4)
df$site_ID[6] = substring(df$site_ID[6],1,4)
df$site_ID[101] = substring(df$site_ID[101],1,3)
df$site_ID[102] = substring(df$site_ID[102],1,3)
df$site_ID[103] = substring(df$site_ID[103],1,3)
df$site_ID[104] = substring(df$site_ID[104],1,3)
df$site_ID[105] = substring(df$site_ID[105],1,3)
df$site_ID[109] = substring(df$site_ID[109],1,3)
df$site_ID[110] = substring(df$site_ID[110],1,3)
df1 = Reduce(function(x,y)merge(x,y, all = T, by = c('site_ID')),list(loc,df,slope_hr)) #
write.csv(df1, "Merged_all_KS_data_032422.csv")
df.all = df1
